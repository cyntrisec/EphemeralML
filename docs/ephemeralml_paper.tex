\documentclass[11pt,a4paper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage[margin=1in]{geometry}
\usepackage{tabularx}
\setlength{\emergencystretch}{2em}

% --- Title ---
% --- Paper benchmark macros (canonical release-gate run, 2026-02-06) ---
% MiniLM-L6: 3 runs at commit 057a85a with KMS fail-closed enforcement
% MiniLM-L12 & BERT-base: 3 runs from benchmark_results_multimodel_20260205
%   (MiniLM-L12 at f0b372a, BERT-base at 8fc6c36)
% Public artifact page: https://github.com/cyntrisec/EphemeralML/releases/tag/v1.0.0
% SHA-256:  20309ab610e7321de5e29d019f0d4b15fee6a7cdafe919686ec1cbd4fabe5937
\newcommand{\BenchInstance}{m6i.xlarge}
\newcommand{\BenchModelName}{MiniLM-L6-v2}
\newcommand{\BenchModelParams}{22.7M}
\newcommand{\BenchNumRuns}{3}
\newcommand{\BenchCommit}{057a85a}
\newcommand{\BenchArtifactReleasePage}{https://github.com/cyntrisec/EphemeralML/releases/tag/v1.0.0}

\newcommand{\BenchBareMeanMs}{79.21}
\newcommand{\BenchEnclaveMeanMs}{89.71}
\newcommand{\BenchOverheadPct}{13.3\%}
\newcommand{\BenchBarePNinetyFiveMs}{80.64}
\newcommand{\BenchEnclavePNinetyFiveMs}{91.62}
\newcommand{\BenchBareThroughput}{12.63}
\newcommand{\BenchEnclaveThroughput}{11.15}
\newcommand{\BenchBarePeakRssMb}{266}
\newcommand{\BenchEnclavePeakRssMb}{586}

\newcommand{\BenchAttestationMs}{153}
\newcommand{\BenchKmsMs}{75}
\newcommand{\BenchSThreeFetchMs}{5440}
\newcommand{\BenchDecryptMs}{101}
\newcommand{\BenchLoadMs}{40}
\newcommand{\BenchTokenizerMs}{24}
\newcommand{\BenchColdStartTotalMs}{5833}

% Multi-model scaling table (3-run means from benchmark_results_multimodel_20260205)
\newcommand{\ModelLTwelveBareMeanMs}{157.25}
\newcommand{\ModelLTwelveEnclaveMeanMs}{177.57}
\newcommand{\ModelLTwelveOverheadPct}{12.9\%}
\newcommand{\ModelLTwelveParams}{33.4M}
\newcommand{\ModelBertBaseBareMeanMs}{65.66}
\newcommand{\ModelBertBaseEnclaveMeanMs}{73.49}
\newcommand{\ModelBertBaseOverheadPct}{11.9\%}
\newcommand{\ModelBertBaseParams}{110M}
\newcommand{\BenchOverheadRange}{11.9--13.3\%}

\title{\textbf{EphemeralML: Confidential AI Inference on AWS Nitro Enclaves\\with \BenchOverheadRange{} Overhead}}
\author{Borys Tsyrulnikov\\Cyntrisec}
\date{February 2026}

\begin{document}
\maketitle

% ============================================================
\begin{abstract}
Running machine learning inference on untrusted cloud infrastructure exposes model weights, input data, and predictions to the infrastructure operator.
We present \textsc{EphemeralML}, a confidential AI inference system that executes ML models inside AWS Nitro Enclaves with end-to-end encryption between client and enclave.
The host operates as a \emph{blind relay}: it forwards encrypted bytes over VSock but can never decrypt or inspect sensitive data, even under full root compromise (content-blind, not metadata-blind).
Our cryptographic protocol combines HPKE session establishment (X25519 + ChaCha20-Poly1305) with AEAD-authenticated session metadata, COSE\slash CBOR attestation verification with ECDSA-P384 certificate chains, attestation-gated KMS key release with per-model encryption context binding, and Ed25519-signed Attested Execution Receipts.
We evaluate \textsc{EphemeralML} on an \texttt{\BenchInstance{}} instance across three encoder models (\BenchModelName{} at \BenchModelParams{}, MiniLM-L12-v2 at \ModelLTwelveParams{}, and BERT-base at \ModelBertBaseParams{} parameters) and measure \textbf{\BenchOverheadRange{} mean inference overhead} across models (\BenchBareMeanMs{}\,ms bare metal vs.\ \BenchEnclaveMeanMs{}\,ms enclave for \BenchModelName{}), near-identical embedding output (cosine similarity $\approx 1.0$, bit-identical within environment across \BenchNumRuns{} runs), a per-inference cryptographic budget of 0.028\,ms for 1\,KB payloads (enclave-side), and an end-to-end per-request crypto overhead of 0.167\,ms excluding inference.
The system is implemented in 13,000+ lines of Rust across four workspace crates, compiles for both local mock and production Nitro modes via feature flags, and is fully open-source with 110+ tests.
\end{abstract}

% ============================================================
\section{Introduction}
\label{sec:introduction}

Machine learning inference is increasingly deployed on shared cloud infrastructure where the operator---or an attacker who compromises the host---can observe model inputs, outputs, and weights.
This is unacceptable for defense, government, financial, and healthcare workloads where both the model intellectual property and the inference data are sensitive.

Trusted Execution Environments (TEEs) offer hardware-enforced isolation that prevents even a privileged host from reading enclave memory.
AWS Nitro Enclaves provide a TEE based on a stripped-down Linux kernel with no persistent storage, no networking, and no interactive access; the only communication channel is a VSock virtual socket to the parent EC2 instance.

We present \textsc{EphemeralML}, a system that leverages Nitro Enclaves to provide confidential AI inference with the following contributions:

\begin{enumerate}
  \item A \textbf{three-zone security architecture} (Client, Host, Enclave) in which the host is explicitly untrusted and operates as a blind relay of encrypted bytes.
  \item A \textbf{cryptographic protocol} that binds HPKE session keys to Nitro attestation documents, authenticates routing metadata as AEAD associated data to prevent cross-session ciphertext splicing, gates model decryption keys through attestation-verified KMS policies with per-model encryption context, and produces Ed25519-signed Attested Execution Receipts for non-repudiation.
  \item A \textbf{production-quality implementation} in 13,000+ lines of Rust that compiles for both local development (mock mode) and Nitro Enclaves (production mode) from the same codebase via feature flags.
  \item A \textbf{comprehensive evaluation} across three encoder models showing \BenchOverheadRange{} inference overhead, sub-millisecond per-request cryptographic cost, and near-identical output quality with high reproducibility (\BenchNumRuns{} runs per model, CV $< 1\%$).
\end{enumerate}

% ============================================================
\section{Threat Model}
\label{sec:threat-model}

\subsection{Trust Assumptions}

\textsc{EphemeralML} operates under the following assumptions:

\begin{description}
  \item[A1 (Client Trust)] The client environment is trusted. It holds the PCR allowlist, policy root, and attestation verification logic.
  \item[A2 (Nitro Attestation Roots)] The AWS Nitro attestation root certificates (Root CA G1) are trusted.
  \item[A3 (Host Compromise)] The host OS is assumed \emph{fully compromised}. An attacker with root access can observe and modify VSock traffic, control scheduling and storage, but \emph{cannot} read enclave memory.
  \item[A4 (Network Adversary)] An on-path attacker can observe and modify all network traffic between client and host.
  \item[A5 (KMS Trust)] AWS KMS correctly enforces attestation-bound key release via RSA-2048 RecipientInfo envelopes.
  \item[A6 (Time Source)] Freshness is established through nonces and challenge-response protocols, not wall-clock time.
  \item[A7 (Side Channels)] Residual timing and access-pattern leakage exists. Mitigations are documented but not fully eliminated in v1.
\end{description}

\subsection{Adversary Capabilities}

Under assumption A3, the adversary can:
\begin{itemize}
  \item Inspect, replay, reorder, or drop all VSock messages between host and enclave.
  \item Control the scheduling of the enclave vCPUs.
  \item Modify or replace files on the host filesystem, including model artifacts in transit.
  \item Deny service by refusing to forward traffic or terminating the enclave.
\end{itemize}

The adversary \emph{cannot}:
\begin{itemize}
  \item Read or modify enclave memory (hardware isolation).
  \item Forge Nitro attestation documents (signed by the Nitro Security Module).
  \item Obtain KMS data keys without a valid attestation document matching the KMS key policy.
  \item Decrypt HPKE-encrypted traffic without the enclave's ephemeral private key.
\end{itemize}

\subsection{Out of Scope (v1)}

Black-box model extraction via repeated queries, complete protection from microarchitectural side-channels, availability guarantees (the host can always deny service), GPU side-channel hardening, and model confidentiality under enclave compromise are explicitly out of scope for the current version.

% ============================================================
\section{System Architecture}
\label{sec:architecture}

\textsc{EphemeralML} uses a three-zone security model illustrated in Figure~\ref{fig:zones}.

\begin{figure}[ht]
\centering
\fbox{\parbox{0.9\textwidth}{
\centering
\textbf{Client Zone} (Trusted)
$\overset{\text{HPKE over TLS}}{\longleftrightarrow}$
\textbf{Host Zone} (Untrusted Relay)
$\overset{\text{HPKE over VSock}}{\longleftrightarrow}$
\textbf{Enclave Zone} (Trusted TEE)
}}
\caption{Three-zone architecture. The host forwards encrypted bytes without decryption capability.}
\label{fig:zones}
\end{figure}

\paragraph{Client Zone.}
The client is a Rust library that performs attestation verification, policy enforcement, HPKE session negotiation, and result decryption.
It maintains a PCR allowlist (SHA-384 hashes of PCR0, PCR1, PCR2) and verifies the COSE\_Sign1 attestation document before establishing an encrypted session.

\paragraph{Host Zone.}
The host runs on the parent EC2 instance and acts as a \emph{blind relay}.
It proxies three types of requests over VSock (port 8082, CID 3):
\begin{itemize}
  \item \textbf{KmsProxy}: Forwards attestation-gated KMS \texttt{Decrypt} and \texttt{GenerateDataKey} requests.
  \item \textbf{Storage}: Fetches encrypted model artifacts from S3 and streams them to the enclave.
  \item \textbf{Audit}: Forwards enclave log messages to the host's logging infrastructure.
\end{itemize}
At no point does the host possess decryption keys or see plaintext model data.

\paragraph{Enclave Zone.}
The enclave runs inside a Nitro Enclave with no network access, no persistent storage, and no interactive shell.
On startup, it:
(1) obtains the model decryption key from KMS (attestation-gated),
(2) fetches and decrypts model artifacts via the host's S3 proxy,
(3) loads the model into the Candle inference engine, and
(4) begins accepting client connections over VSock.%
\footnote{The v1.0 prototype verifies S3 connectivity and KMS key release at boot but defers model registration to the benchmark binary; the production \texttt{main.rs} boot sequence is a roadmap item (Phase~1).}
For each client session, it generates a fresh attestation document via the Nitro Security Module (NSM), creates ephemeral X25519 and Ed25519 key pairs (embedding the public keys in the attestation user data), and establishes an HPKE-encrypted channel before serving inference requests.

% ============================================================
\section{Cryptographic Protocol}
\label{sec:protocol}

\subsection{HPKE Session Establishment}

\textsc{EphemeralML} uses a fixed HPKE cipher suite:
\begin{itemize}
  \item \textbf{KEM}: X25519 (\texttt{0x0020})
  \item \textbf{KDF}: HKDF-SHA256 (\texttt{0x0001})
  \item \textbf{AEAD}: ChaCha20-Poly1305 (\texttt{0x0003})
\end{itemize}

Session keys are derived by binding the Diffie-Hellman shared secret to the attestation transcript:

\begin{equation}
\label{eq:transcript}
H_{\mathrm{transcript}} = \mathrm{SHA256}\!\left(
  H_{\mathrm{attest}} \;\|\;
  \mathrm{sort}(pk_{\mathrm{local}}, pk_{\mathrm{peer}}) \;\|\;
  n_{\mathrm{client}} \;\|\;
  v_{\mathrm{proto}}
\right)
\end{equation}

\begin{equation}
\label{eq:session-key}
k_{\mathrm{session}} = \mathrm{HKDF\text{-}SHA256}\!\left(
  \mathrm{salt}{=}\bot,\;
  \mathrm{ikm}{=}\mathrm{X25519\text{-}DH}(sk_{\mathrm{local}}, pk_{\mathrm{peer}}),\;
  \mathrm{info}{=}H_{\mathrm{transcript}},\;
  L{=}32
\right)
\end{equation}

The sorted public key ordering ensures both sides derive the same transcript hash regardless of role.

\subsection{Message Framing, AAD Binding, and Replay Protection}

Each encrypted message uses the following wire format inside the AEAD ciphertext:

\begin{center}
\begin{tabular}{llc}
\hline
\textbf{Field} & \textbf{Type} & \textbf{Bytes} \\
\hline
Protocol version & u32 (big-endian) & 4 \\
Session ID length & u32 (big-endian) & 4 \\
Session ID & bytes & variable \\
Sequence number & u64 (big-endian) & 8 \\
Plaintext length & u32 (big-endian) & 4 \\
Plaintext & bytes & variable \\
\hline
\end{tabular}
\end{center}

The per-message nonce for ChaCha20-Poly1305 is derived as:

\begin{equation}
\label{eq:nonce}
\mathrm{nonce} = \mathrm{SHA256}\!\left(
  k_{\mathrm{session}} \;\|\;
  \mathrm{seq} \;\|\;
  H_{\mathrm{transcript}} \;\|\;
  \texttt{"ChaCha20Poly1305-Nonce"}
\right)[0..12]
\end{equation}

\paragraph{AEAD Associated Data.}
The outer \texttt{EncryptedMessage} fields (protocol version, session ID, and sequence number) are passed as Associated Authenticated Data (AAD) to ChaCha20-Poly1305:

\begin{equation}
\label{eq:aad}
\mathrm{AAD} = v_{\mathrm{proto}} \;\|\;
  \mathrm{len}(\mathrm{sid}) \;\|\;
  \mathrm{sid} \;\|\;
  \mathrm{seq}
\end{equation}

This binds the routing metadata to the AEAD authentication tag, so a hostile relay that attempts to splice ciphertext between sessions or reorder messages will cause an authentication failure on decryption.
The inner message frame also contains these fields as defense-in-depth, but the AAD binding provides a cryptographic guarantee at the AEAD level independent of the plaintext contents.

\paragraph{Replay Protection.}
Replay protection is enforced by requiring strictly monotonically increasing sequence numbers on both sides.
Each side maintains an independent outgoing and incoming sequence counter; a message is rejected unless its sequence number exactly matches the expected next incoming value.

\subsection{Attestation Verification}

The client verifies the Nitro attestation document through the following steps:

\begin{enumerate}
  \item Parse the COSE\_Sign1 structure and extract the certificate chain from the CBOR payload fields (\texttt{certificate} and \texttt{cabundle}).
  \item Verify the ECDSA-P384-SHA384 signature over the \texttt{Sig\_structure}:
  \[
    \texttt{Sig\_structure} = [\texttt{"Signature1"},\; \mathrm{protected},\; \mathrm{external\_aad},\; \mathrm{payload}]
  \]
  \item Walk the certificate chain from the AWS Nitro Root CA (G1) through intermediates to the leaf, verifying each signature.
  \item Extract and validate PCR measurements (PCR0, PCR1, PCR2; SHA-384, 48 bytes each) against the client's allowlist.
  \item Extract the \texttt{user\_data} field containing the enclave's ephemeral HPKE public key (X25519, 32 bytes), Ed25519 receipt-signing public key (32 bytes), protocol version, and supported features.
\end{enumerate}

\subsection{Attestation-Gated KMS Key Release}

Model artifacts are encrypted with a data encryption key (DEK) that is itself encrypted under a KMS customer master key (CMK).
The KMS key policy requires a valid Nitro attestation document with specific PCR values.
The enclave presents its attestation document to KMS (via the host's blind KMS proxy) and receives the DEK only if the measurements match.

To prevent cross-model ciphertext replay, the enclave includes a KMS \emph{encryption context} containing the model identifier and version in every \texttt{Decrypt} call:

\begin{verbatim}
  encryption_context: {
    "model_id": "<manifest.model_id>",
    "version":  "<manifest.version>"
  }
\end{verbatim}

KMS requires the encryption context at decrypt time to match the context supplied at encrypt time; a ciphertext blob encrypted for one model cannot be replayed to decrypt a different model's DEK.

\subsection{Attested Execution Receipts}

After each inference, the enclave produces an Attested Execution Receipt (AER) containing:

\begin{itemize}
  \item Receipt ID, protocol version, and security mode
  \item Enclave measurements (PCR0/1/2) and attestation document hash
  \item SHA-256 hashes of the request and response
  \item Model ID, version, and execution time
  \item Monotonic sequence number
\end{itemize}

The receipt is CBOR-encoded with deterministic field ordering, then signed with the enclave's ephemeral Ed25519 private key (64-byte signature).
The client verifies the signature using the Ed25519 public key extracted from the attestation document.

% ============================================================
\section{Implementation}
\label{sec:implementation}

\textsc{EphemeralML} is implemented in 13,000+ lines of Rust (2021 edition, stable toolchain, MSRV 1.75+) organized as a Cargo workspace with four core crates (plus a VSock benchmarking utility):

\begin{description}
  \item[\texttt{common/}] Shared cryptographic primitives (HPKE sessions, receipt signing), protocol definitions, VSock abstractions, input validation, and policy types.
  \item[\texttt{client/}] Client library providing attestation verification, policy enforcement, model validation, freshness checking, and the secure client API.
  \item[\texttt{host/}] Host relay proxy implementing KMS proxy, S3 storage forwarding, circuit breaker, metrics collection, and OpenTelemetry tracing.
  \item[\texttt{enclave/}] Enclave application with NSM attestation, Candle-based inference engine, KMS client, model loader, and audit logging.
\end{description}

\paragraph{Feature-Gated Dual Mode.}
The same codebase compiles for two targets via Cargo feature flags:
\begin{itemize}
  \item \textbf{\texttt{mock}} (default): Simulates NSM attestation and VSock for local development and testing.
  \item \textbf{\texttt{production}}: Uses the real NSM API and production VSock transport.
\end{itemize}

\paragraph{Inference Engine.}
Model inference uses Candle (\texttt{candle-core}~0.9, \texttt{candle-nn}~0.9).
Weights are deserialized with SafeTensors~0.7 and tokenization uses HuggingFace Tokenizers~0.21.
The \texttt{cuda} feature flag enables GPU acceleration where available.

\paragraph{Security Practices.}
All key material types derive \texttt{ZeroizeOnDrop} for automatic memory clearing.
We rely on constant-time implementations in the underlying cryptographic libraries for AEAD tag checks and signature verification.
Input validation is performed at all system boundaries.
Error types are hierarchical (\texttt{EphemeralError}, \texttt{HostError}, \texttt{EnclaveError}) with \texttt{?}-propagation throughout.

\paragraph{Testing.}
The test suite comprises 110+ tests covering unit tests (inline \texttt{\#[cfg(test)]} modules), integration tests (\texttt{tests/} directories), and end-to-end protocol tests.
The CI pipeline runs formatting, clippy, and test jobs on every push to \texttt{main}; these core jobs pass for the release-gate commit.

% ============================================================
\section{Evaluation}
\label{sec:evaluation}

We evaluate \textsc{EphemeralML} on an \texttt{\BenchInstance{}} EC2 instance (4~vCPUs, 16\,GB RAM, Intel Xeon Ice Lake) running Amazon Linux~2023.
The enclave is allocated 2~vCPUs and 4096\,MB RAM.
We benchmark three encoder models: \BenchModelName{} (\BenchModelParams{} parameters, 384-dim embeddings), MiniLM-L12-v2 (\ModelLTwelveParams{}, 384-dim), and BERT-base-uncased (\ModelBertBaseParams{}, 768-dim).
All latency measurements use 100 iterations after 3 warmup runs, repeated \BenchNumRuns{} times per model.
The primary MiniLM-L6-v2 results reported below are from a single-commit release-gate run at \texttt{\BenchCommit{}} with KMS fail-closed enforcement verified per run (see Artifact Provenance, Section~\ref{sec:provenance}).
Multi-model scaling data (MiniLM-L12-v2 and BERT-base) were collected in a separate campaign (MiniLM-L12-v2 at \texttt{f0b372a}, BERT-base at \texttt{8fc6c36}; inference logic unchanged between commits).
Results are highly reproducible: coefficient of variation is $< 1\%$ for both bare-metal and enclave inference latency across all models and runs.

\subsection{Inference Latency}

Table~\ref{tab:inference} shows the core inference overhead.

\begin{table}[ht]
\centering
\caption{Inference latency comparison (\BenchModelName{}, \BenchInstance{}, \BenchNumRuns{}-run average).}
\label{tab:inference}
\begin{tabular}{lrrr}
\hline
\textbf{Metric} & \textbf{Bare Metal} & \textbf{Enclave} & \textbf{Overhead} \\
\hline
Mean latency     & \BenchBareMeanMs{}\,ms  & \BenchEnclaveMeanMs{}\,ms  & +\BenchOverheadPct{} \\
P95 latency      & \BenchBarePNinetyFiveMs{}\,ms  & \BenchEnclavePNinetyFiveMs{}\,ms  & +13.6\% \\
Throughput       & \BenchBareThroughput{}\,inf/s & \BenchEnclaveThroughput{}\,inf/s & $-$11.7\% \\
Peak RSS (VmHWM) & \BenchBarePeakRssMb{}\,MB    & \BenchEnclavePeakRssMb{}\,MB & +120\% \\
\hline
\end{tabular}
\end{table}

The \BenchOverheadPct{} mean inference overhead for \BenchModelName{} falls within the ``Acceptable'' range and is consistent with published AMD SEV-SNP overhead for TensorFlow BERT (2.3--16\%, depending on VM and polling configuration)~\cite{misono2024}.
Peak RSS is measured from \texttt{/proc/self/status} (VmHWM); the enclave's 2.2$\times$ memory overhead is attributable to the enclave kernel and VSock message buffers.

\subsection{Overhead vs.\ Model Size (Encoder Models)}

Table~\ref{tab:model-scaling} summarizes overhead across three encoder-style models,
each benchmarked \BenchNumRuns{} times with 100 iterations per run.
Overhead remains in the \BenchOverheadRange{} range regardless of model size, from 22.7M to 110M parameters.

\begin{table}[ht]
\centering
\caption{Overhead scaling across encoder models (\BenchInstance{}, \BenchNumRuns{}-run averages).}
\label{tab:model-scaling}
\begin{tabular}{lrrrrr}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Bare Metal} & \textbf{Enclave} & \textbf{Overhead} & \textbf{EN RSS} \\
\hline
\BenchModelName{} & \BenchModelParams{} & \BenchBareMeanMs{}\,ms & \BenchEnclaveMeanMs{}\,ms & +\BenchOverheadPct{} & \BenchEnclavePeakRssMb{}\,MB \\
MiniLM-L12-v2 & \ModelLTwelveParams{} & \ModelLTwelveBareMeanMs{}\,ms & \ModelLTwelveEnclaveMeanMs{}\,ms & +\ModelLTwelveOverheadPct{} & 1{,}494\,MB \\
BERT-base & \ModelBertBaseParams{} & \ModelBertBaseBareMeanMs{}\,ms & \ModelBertBaseEnclaveMeanMs{}\,ms & +\ModelBertBaseOverheadPct{} & 2{,}826\,MB \\
\hline
\end{tabular}
\end{table}

Notably, BERT-base (110M parameters) has a lower overhead percentage (+\ModelBertBaseOverheadPct{}) than \BenchModelName{} (+\BenchOverheadPct{}) despite being $\sim$4.8$\times$ larger, suggesting that enclave overhead is partly fixed-cost (kernel, VSock buffers) rather than proportional to model size.

\subsection{Cold Start Breakdown}

Table~\ref{tab:coldstart} decomposes the enclave cold start latency.

\begin{table}[ht]
\centering
\caption{Cold start breakdown (enclave).}
\label{tab:coldstart}
\begin{tabular}{lr}
\hline
\textbf{Phase} & \textbf{Latency} \\
\hline
NSM attestation generation & \BenchAttestationMs{}\,ms \\
KMS key release            & \BenchKmsMs{}\,ms \\
S3 model fetch (via VSock) & \BenchSThreeFetchMs{}\,ms \\
Model decryption           & \BenchDecryptMs{}\,ms \\
Model load (Candle)        & \BenchLoadMs{}\,ms \\
Tokenizer setup            & \BenchTokenizerMs{}\,ms \\
\hline
\textbf{Total}             & \textbf{\BenchColdStartTotalMs{}\,ms} \\
\hline
\end{tabular}
\end{table}

Cold start is dominated by the S3 model fetch over VSock (93\% of total).
This could be reduced by embedding weights in the EIF image or implementing a pre-warming protocol.
The one-time attestation and KMS costs (\BenchAttestationMs{} + \BenchKmsMs{} $\approx$ 228\,ms combined) are negligible for session-based workloads.
Attestation latency varies between runs (94--300\,ms) due to NSM warm-up effects.

\subsection{VSock Round-Trip Time}

Table~\ref{tab:vsock} shows VSock latency for varying payload sizes measured using the Audit message channel.

\begin{table}[ht]
\centering
\caption{VSock round-trip time by payload size.}
\label{tab:vsock}
\begin{tabular}{rr}
\hline
\textbf{Payload} & \textbf{RTT} \\
\hline
64\,B   & 0.18\,ms \\
1\,KB   & 0.17\,ms \\
64\,KB  & 0.42\,ms \\
1\,MB   & 4.40\,ms \\
\hline
\multicolumn{2}{l}{Upload throughput: 227\,MB/s} \\
\hline
\end{tabular}
\end{table}

\subsection{Cryptographic Primitive Performance}

Table~\ref{tab:crypto} measures the cost of individual cryptographic operations.

\begin{table}[ht]
\centering
\caption{Cryptographic primitive latency (bare metal m6i.xlarge).}
\label{tab:crypto}
\begin{tabular}{lrr}
\hline
\textbf{Operation} & \textbf{Mean} & \textbf{P99} \\
\hline
HPKE session setup (both sides) & 0.10\,ms & 0.13\,ms \\
X25519 keypair generation       & 0.017\,ms & 0.023\,ms \\
HPKE encrypt 1\,KB              & 0.003\,ms & 0.003\,ms \\
HPKE decrypt 1\,KB              & 0.003\,ms & 0.003\,ms \\
HPKE encrypt 1\,MB              & 1.35\,ms  & 1.70\,ms \\
HPKE decrypt 1\,MB              & 1.29\,ms  & 1.55\,ms \\
Ed25519 keypair generation      & 0.017\,ms & 0.019\,ms \\
Receipt sign (CBOR + Ed25519)   & 0.022\,ms & 0.022\,ms \\
Receipt verify                  & 0.042\,ms & 0.047\,ms \\
\hline
\textbf{Per-inference crypto (1\,KB)} & \textbf{0.028\,ms} & --- \\
\hline
\end{tabular}
\end{table}

The per-inference cryptographic budget of 0.028\,ms represents $\sim$0.04\% of the \BenchBareMeanMs{}\,ms inference latency, confirming that encryption overhead is negligible.

\subsection{COSE Attestation Verification}

Table~\ref{tab:cose} shows the client-side cost of verifying a Nitro attestation document.

\begin{table}[ht]
\centering
\caption{COSE attestation verification latency (client-side, bare metal m6i.xlarge).}
\label{tab:cose}
\begin{tabular}{lrr}
\hline
\textbf{Operation} & \textbf{Mean} & \textbf{P99} \\
\hline
COSE\_Sign1 verify (ECDSA-P384) & 0.740\,ms & 0.770\,ms \\
Certificate chain walk (3 certs) & 2.219\,ms & 2.263\,ms \\
CBOR payload parse               & 0.001\,ms & 0.001\,ms \\
\hline
\textbf{Full verification pipeline} & \textbf{2.960\,ms} & \textbf{3.034\,ms} \\
\hline
\end{tabular}
\end{table}

Attestation verification is a one-time cost per session (amortized to zero for long-lived sessions).

\subsection{End-to-End Encrypted Request Overhead}

Table~\ref{tab:e2e} shows the per-request overhead of the full encrypted inference pipeline.

\begin{table}[ht]
\centering
\caption{End-to-end encrypted request overhead (bare metal m6i.xlarge).}
\label{tab:e2e}
\begin{tabular}{lr}
\hline
\textbf{Component} & \textbf{Mean Latency} \\
\hline
Per-request crypto (encrypt + decrypt + receipt) & 0.167\,ms \\
Session setup (one-time)                         & 0.138\,ms \\
TCP handshake (one-time)                         & 0.157\,ms \\
\hline
\end{tabular}
\end{table}

\subsection{Concurrency Scaling}

Table~\ref{tab:concurrency} evaluates throughput scaling with parallel inference threads on bare metal (50 iterations per thread).

\begin{table}[ht]
\centering
\caption{Concurrency scaling (bare metal m6i.xlarge, 4 vCPUs, 50 iterations/thread).}
\label{tab:concurrency}
\begin{tabular}{rrrr}
\hline
\textbf{Threads} & \textbf{Throughput} & \textbf{Mean Latency} & \textbf{Efficiency} \\
\hline
1 & 12.61\,inf/s & 79.3\,ms  & 100\% \\
2 & 14.49\,inf/s & 138.0\,ms & 57.5\% \\
4 & 14.38\,inf/s & 273.4\,ms & 28.5\% \\
8 & 14.35\,inf/s & 554.0\,ms & 14.2\% \\
\hline
\end{tabular}
\end{table}

Throughput plateaus at 2 threads ($\sim$14.5\,inf/s), indicating that MiniLM inference on this instance is compute-bound on the available 4 vCPUs.
The model's GEMM operations already saturate available cores at 2 threads, so additional threads increase latency without improving throughput.

\subsection{Output Quality}

We compare the embedding vectors produced by bare-metal and enclave inference for identical inputs across all \BenchNumRuns{} runs.
Within each environment, the reference embedding SHA-256 is \emph{bit-identical} across all runs (1 unique hash for bare metal, 1 for enclave), demonstrating fully deterministic output under fixed inputs and software versions.
Cross-environment comparisons show near-identical embeddings: cosine similarity = 0.999999999999871 with maximum absolute difference $5.8 \times 10^{-7}$, attributable to f32 precision differences between bare-metal and enclave kernels.

\subsection{Input Size Scaling}

Table~\ref{tab:input-scaling} measures inference latency as a function of input token count (bare metal, padding disabled).

\begin{table}[ht]
\centering
\caption{Inference latency vs.\ input token count (\BenchModelName{}, bare metal \BenchInstance{}).}
\label{tab:input-scaling}
\begin{tabular}{rrrrr}
\hline
\textbf{Tokens} & \textbf{Mean} & \textbf{P50} & \textbf{P95} & \textbf{P99} \\
\hline
32  & 22.90\,ms  & 22.75\,ms  & 24.55\,ms  & 25.49\,ms \\
63  & 41.35\,ms  & 41.46\,ms  & 43.81\,ms  & 44.90\,ms \\
128 & 86.68\,ms  & 86.68\,ms  & 88.37\,ms  & 89.37\,ms \\
256 & 233.53\,ms & 232.90\,ms & 238.79\,ms & 239.78\,ms \\
\hline
\end{tabular}
\end{table}

The linear fit is $\mathrm{latency} = -18.1\,\text{ms} + 0.953\,\text{ms/token}$.
While BERT self-attention is theoretically quadratic in sequence length, the empirical fit is near-linear over the 32--256 token range tested, likely because GEMM constants and memory access patterns dominate at these moderate lengths.

\subsection{True End-to-End Latency}

Table~\ref{tab:true-e2e} shows the full request path including both cryptographic operations and real BERT inference: HPKE encrypt $\rightarrow$ decrypt $\rightarrow$ inference $\rightarrow$ receipt sign/verify $\rightarrow$ HPKE encrypt response.

\begin{table}[ht]
\centering
\caption{True end-to-end latency: crypto + inference (\BenchModelName{}, bare metal \BenchInstance{}).}
\label{tab:true-e2e}
\begin{tabular}{lrrrr}
\hline
\textbf{Component} & \textbf{Mean} & \textbf{P50} & \textbf{P95} & \textbf{P99} \\
\hline
Session setup (one-time) & 0.15\,ms & 0.14\,ms & 0.17\,ms & 0.18\,ms \\
Per-request E2E          & 79.30\,ms & 79.26\,ms & 80.53\,ms & 81.46\,ms \\
Inference only           & 79.00\,ms & 78.97\,ms & 80.20\,ms & 81.14\,ms \\
\hline
\multicolumn{5}{l}{\textbf{Crypto overhead per request: 0.300\,ms} (0.38\% of E2E)} \\
\hline
\end{tabular}
\end{table}

The 0.300\,ms crypto overhead per request confirms that HPKE encryption, decryption, and receipt signing/verification add negligible latency compared to model inference.

\subsection{Enclave Concurrency}

Table~\ref{tab:enclave-concurrency} evaluates throughput scaling with concurrent end-to-end sessions (each with independent HPKE channels, attestation, and receipt verification) on bare metal.

\begin{table}[ht]
\centering
\caption{Enclave concurrency scaling: full E2E path per request (\BenchModelName{}, bare metal \BenchInstance{}).}
\label{tab:enclave-concurrency}
\begin{tabular}{rrrr}
\hline
\textbf{Clients} & \textbf{Throughput} & \textbf{Mean Latency} & \textbf{Efficiency} \\
\hline
1 & 12.60\,inf/s & 79.38\,ms  & 100\% \\
2 & 14.42\,inf/s & 138.69\,ms & 57.2\% \\
4 & 14.35\,inf/s & 275.32\,ms & 28.5\% \\
\hline
\end{tabular}
\end{table}

The results mirror the raw concurrency scaling (Table~\ref{tab:concurrency}), confirming that per-session HPKE and receipt overhead is negligible even under contention.

\subsection{Reproducibility}

Table~\ref{tab:reproducibility} summarizes the coefficient of variation (CV\%) across \BenchNumRuns{} independent runs for each model, where each run comprises 100 iterations after 3 warmup iterations.

\begin{table}[ht]
\centering
\caption{Reproducibility: coefficient of variation across \BenchNumRuns{} runs per model (\BenchInstance{}).}
\label{tab:reproducibility}
\begin{tabular}{lrrr}
\hline
\textbf{Model} & \textbf{BL CV\%} & \textbf{EN CV\%} & \textbf{Overhead CV\%} \\
\hline
\BenchModelName{} & 0.1\% & 0.3\% & 3.9\% \\
MiniLM-L12-v2     & 0.7\% & 0.7\% & 9.9\% \\
BERT-base          & 0.3\% & 0.2\% & 4.2\% \\
\hline
\end{tabular}
\end{table}

All inference latency measurements show CV $< 1\%$, indicating high stability.
The overhead CV\% is higher (4--10\%) because it amplifies measurement noise: when two nearly equal quantities are subtracted, small absolute variations produce larger relative variations.
Despite this, the overhead consistently falls within the \BenchOverheadRange{} range across all runs and models.

\subsection{Cost Analysis}

Table~\ref{tab:cost} shows the per-inference cost on \texttt{m6i.xlarge} (\$0.192/hr on-demand as of 2026-02-06).

\begin{table}[ht]
\centering
\caption{Cost analysis (m6i.xlarge, \$0.192/hr on-demand pricing as of 2026-02-06).}
\label{tab:cost}
\begin{tabular}{lrr}
\hline
\textbf{Metric} & \textbf{Bare Metal} & \textbf{Enclave} \\
\hline
Inferences/hour        & 45{,}468 & 40{,}140 \\
Cost per 1M inferences & \$4.22 & \$4.78 \\
Cost multiplier        & ---    & 1.13$\times$ \\
\hline
\end{tabular}
\end{table}

The 13\% cost premium for confidential inference is modest, especially given that the enclave provides hardware-enforced isolation, cryptographic attestation, and end-to-end encryption.

% ============================================================
\section{Related Work}
\label{sec:related-work}

\paragraph{TEE-Based ML Inference.}
Intel SGX has been used for confidential inference in systems such as Slalom~\cite{tramer2019} and Occlumency~\cite{lee2019}, but SGX's limited Enclave Page Cache (128\,MB in SGXv1) restricts model size and causes heavy paging overhead for large models.
AMD SEV-SNP provides VM-level isolation with lower overhead; Misono et~al.~\cite{misono2024} report 2.3--16\% overhead for TensorFlow BERT inference on SEV-SNP (varying with VM and polling configuration), consistent with our \BenchOverheadRange{} result on Nitro Enclaves across three encoder models (22.7M--110M parameters).
However, overhead scales with model size: Schnabl et~al.~\cite{schnabl2025} report $2.2\times$ cost overhead for Llama-3.1-8B-Instruct (8 billion parameters, 4-bit quantized) running inside Nitro Enclaves, and note that the magnitude is sensitive to instance sizing (larger instances can reduce overhead).
This suggests that our \BenchOverheadRange{} result, while well-validated for embedding-class models across multiple sizes, should not be extrapolated to billion-parameter LLMs without further evaluation.
Intel TDX offers similar VM-level confidential computing; Intel reports $\sim$4\% overhead for TensorFlow BERT on 4th Gen Xeon~\cite{intel2023}.

\paragraph{GPU TEEs.}
NVIDIA's Confidential Computing (cGPU) with H100 GPUs enables TEE-protected GPU inference.
Zhu et~al.~\cite{zhu2024} benchmark H100 confidential computing and report $\sim$5--7\% TPS throughput overhead for Llama-3.1-8B (6.9\%) and $\sim$4.6\% for Phi-3-14B, with negligible overhead for Llama-3.1-70B, though the threat model differs (GPU memory is trusted, CPU-GPU transfers are encrypted).
Graviton~\cite{volos2018} and HIX~\cite{jang2019} proposed earlier GPU isolation schemes with higher overhead.

\paragraph{Nitro Enclave Systems.}
AWS Nitro Enclaves have been used for key management (AWS ACM for Nitro Enclaves) and secure computation (Anjuna, Fortanix).
Nitriding~\cite{winter2023} provides a general-purpose toolkit that abstracts away the constrained Nitro Enclave development model, enabling unmodified applications with Internet connectivity and user-verifiable code; however, it does not address ML inference, cryptographic session management, or attested execution receipts.
Lutsch et~al.~\cite{lutsch2025} analyze Nitro Enclave overhead for analytical database workloads, highlighting communication limitations that are consistent with the communication-heavy behavior in \textsc{EphemeralML}'s cold start path.
Schnabl et~al.~\cite{schnabl2025} use Nitro Enclaves for verifiable AI safety auditing with transparency logs, demonstrating a complementary use case (benchmarking rather than serving) with a similar attestation-based trust model.
To our knowledge, \textsc{EphemeralML} is the first open-source system providing end-to-end confidential ML \emph{inference serving} with HPKE session management, attestation-gated key release, and attested execution receipts on Nitro Enclaves.

\paragraph{Cryptographic Inference.}
Fully homomorphic encryption (FHE) and secure multi-party computation (MPC) can provide inference without trusting any party, but incur orders-of-magnitude overhead: CryptoNets~\cite{gilad2016} requires $\sim$250 seconds for a single MNIST prediction under FHE, making such approaches impractical for latency-sensitive workloads.
\textsc{EphemeralML}'s TEE-based approach trades the stronger cryptographic guarantees of FHE/MPC for practical performance.

% ============================================================
\section{Discussion and Future Work}
\label{sec:discussion}

\paragraph{Cold Start Optimization.}
The \BenchColdStartTotalMs{}\,ms cold start is dominated by fetching the model over VSock from S3 (\BenchSThreeFetchMs{}\,ms).
Three mitigation strategies are planned:
(1) embedding model weights directly in the EIF image (eliminating the S3 fetch entirely),
(2) implementing a pre-warming protocol that begins model loading before the first client request, and
(3) using model quantization (INT4/INT8 GGUF format) to reduce transfer size.

\paragraph{GPU TEE Support.}
As NVIDIA H100 cGPU support matures, \textsc{EphemeralML} could extend to GPU-accelerated inference within a TEE.
The Candle framework already supports CUDA backends via the \texttt{cuda} feature flag, and the blind relay architecture is agnostic to the compute backend.

\paragraph{Shield Mode.}
The current v1 implements ``Gateway Only'' mode where the enclave processes inference but does not protect against model extraction via repeated queries.
A planned ``Shield Mode'' would add query budgets, differential privacy noise injection, and rate limiting at the enclave level.

\paragraph{Memory Overhead.}
Enclave memory overhead scales with model size: \BenchEnclavePeakRssMb{}\,MB for \BenchModelName{} (86.7\,MB weights), 1{,}494\,MB for MiniLM-L12-v2 (127\,MB weights), and 2{,}826\,MB for BERT-base (440\,MB weights).
The $\sim$2.2$\times$ ratio of enclave RSS to bare-metal RSS is consistent across models, attributable to the enclave kernel, VSock message decode buffers, and Candle runtime overhead within the enclave.
Optimizing the enclave image (e.g., using a minimal init process, pre-allocating buffers, streaming model loading) could reduce this overhead.

\paragraph{Multi-Model Serving.}
The current architecture loads one model per enclave.
Supporting multiple models would require either multiple enclaves (with separate attestation per model) or a model-switching protocol within a single enclave.

\paragraph{Concurrency.}
Throughput saturates at $\sim$14.5\,inf/s with 2+ threads on 4 vCPUs (Tables~\ref{tab:concurrency},~\ref{tab:enclave-concurrency}), with identical scaling whether measuring raw inference or full E2E sessions with HPKE and receipts.
Larger instances (e.g., \texttt{m6i.4xlarge} with 16 vCPUs) or batched inference could improve throughput scaling.

\paragraph{Transparency Log for Receipts.}
Attested Execution Receipts are currently returned directly to the requesting client, providing non-repudiation only between client and enclave.
For regulated sectors requiring third-party auditability, the receipts could be published to an append-only transparency log, as proposed by Schnabl et~al.~\cite{schnabl2025} for AI safety audits.
This would allow regulators and auditors to independently verify that specific inferences occurred within an attested enclave without requiring cooperation from the client or operator.

\paragraph{Reproducible Builds and PCR Verifiability.}
The client verifies enclave identity through PCR measurements (PCR0, PCR1, PCR2), but the current build process does not guarantee reproducibility---independent parties cannot rebuild the EIF image and confirm it produces the same PCR0 hash.
Following the approach of Nitriding~\cite{winter2023}, which emphasizes user-verifiable enclave code, a reproducible build pipeline (pinned base image digests, deterministic Dockerfile, CI-published PCR values) would close this trust gap and allow clients to independently verify the enclave binary.

\paragraph{Overhead Scaling with Model Size.}
Our multi-model evaluation shows that inference overhead remains in a narrow \BenchOverheadRange{} band across encoder models spanning 22.7M to 110M parameters (Table~\ref{tab:model-scaling}).
The slight decrease from +\BenchOverheadPct{} (\BenchModelName{}) to +\ModelBertBaseOverheadPct{} (BERT-base) suggests that the enclave's fixed costs (kernel, VSock buffers) are amortized over longer per-inference compute times for larger models.
However, published results for Llama-3.1-8B on Nitro Enclaves show $2.2\times$ cost overhead~\cite{schnabl2025}, with the authors noting that the magnitude is sensitive to instance sizing and can be reduced with larger instances; even so, confidential inference at the billion-parameter scale remains substantially more expensive than small encoder models.
Evaluating \textsc{EphemeralML} on billion-parameter models---potentially with INT4/INT8 quantization to fit within enclave memory constraints---is an important direction for validating the system's applicability beyond embedding-class models.

\paragraph{Threats to Validity.}
Our strongest evidence is for encoder-style CPU inference on \texttt{m6i.xlarge}.
Although the multi-model campaign uses commits (\texttt{f0b372a}, \texttt{8fc6c36}) where inference logic was unchanged relative to \texttt{\BenchCommit{}}, non-inference drift between commits cannot be fully excluded.
The current measurements also do not directly generalize to decoder-only billion-parameter LLM serving or GPU-backed confidential inference, where memory reservation and CPU--GPU transfer overheads can dominate.

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \textsc{EphemeralML}, a confidential AI inference system that runs ML models inside AWS Nitro Enclaves with the host operating as a blind relay of encrypted data.
Our evaluation across three encoder models (22.7M--110M parameters), each benchmarked \BenchNumRuns{} times, demonstrates that hardware-enforced confidential inference is practical:
\BenchOverheadRange{} mean inference overhead (CV $< 1\%$ across runs), 0.028\,ms per-inference cryptographic cost, bit-identical output within environments, near-identical output across environments (cosine similarity $\approx 1.0$), and a $\sim$13\% cost premium over bare-metal execution.
Notably, overhead remains stable or decreases slightly as model size increases, suggesting that the enclave's fixed costs are amortized over longer inference times.

The system's three-zone architecture, transcript-bound HPKE sessions, attestation-gated key management, and Ed25519-signed execution receipts provide defense-in-depth against a fully compromised host.
The implementation in 13,000+ lines of Rust, with feature-gated dual compilation and 110+ passing tests, demonstrates that confidential ML inference can be achieved with production-quality engineering.

\textsc{EphemeralML} is open-source and available at GitHub:
\newline
\texttt{https://github.com/cyntrisec/EphemeralML}.

% ============================================================
\section*{Artifact Provenance}
\label{sec:provenance}

The primary MiniLM-L6-v2 benchmark results (Tables~\ref{tab:inference}--\ref{tab:cost} except the multi-model scaling table) are produced by a single release-gate run with KMS fail-closed enforcement.
All \BenchNumRuns{} runs executed at commit \texttt{\BenchCommit{}} on the same \texttt{\BenchInstance{}} instance on 2026-02-06.
Each run verified \texttt{kms\_exercised=true} and \texttt{kms\_bypassed=false}, confirming that attested KMS key release was exercised and not bypassed.
Enclave benchmarks run with \texttt{-{}-debug-mode} to capture console output; this zeroes PCR values in the attestation document, so the benchmark KMS path uses a permissive key policy.
PCR-conditioned key release was validated separately in a non-debug enclave at the same commit (see \texttt{README.md}, ``KMS Attestation Audit Results'').

\begin{description}
  \item[Source code] \url{https://github.com/cyntrisec/EphemeralML}, commit \texttt{\BenchCommit{}}
  \item[Public artifact page]
  \url{\BenchArtifactReleasePage{}}
  \item[Artifact file]
  \texttt{kms\_validation\_20260205\_234917.tar.gz}
  \item[Public reproducibility materials]
  \url{https://github.com/cyntrisec/EphemeralML}
  \newline
  \texttt{benchmark\_results\_multimodel\_20260205/}
  \newline
  \texttt{scripts/}
  \item[Archive SHA-256]
  \texttt{20309ab610e7321de5e29d019f0d4b15}
  \newline
  \texttt{fee6a7cdafe919686ec1cbd4fabe5937}
  \item[Instance] \texttt{\BenchInstance{}} (4~vCPUs, 16\,GB, Intel Xeon Ice Lake), enclave: 2~vCPUs, 4096\,MB
  \item[Model] \BenchModelName{} (\BenchModelParams{} parameters)
  \item[Runs] \BenchNumRuns{} $\times$ 100 iterations (3 warmup), \texttt{require\_kms=true}
  \item[Canonical manifest] \texttt{final\_validation\_manifest.json} inside the archive
\end{description}

Multi-model scaling data (MiniLM-L12-v2, BERT-base) were collected in a separate campaign under the same methodology but at earlier commits (\texttt{f0b372a} and \texttt{8fc6c36} respectively; inference logic unchanged).

% ============================================================
\begin{thebibliography}{10}

\bibitem{misono2024}
T.~Misono, D.~Stavrakakis, N.~Santos, and P.~Bhatotia,
``Confidential VMs explained: An empirical analysis of AMD SEV-SNP and Intel TDX,''
in \emph{Proc.\ ACM on Measurement and Analysis of Computing Systems (SIGMETRICS)}, vol.~8, no.~3, 2024.
\url{https://doi.org/10.1145/3700418}

\bibitem{tramer2019}
F.~Tram\`er and D.~Boneh,
``Slalom: Fast, verifiable and private execution of neural networks in trusted hardware,''
in \emph{Proc.\ ICLR}, 2019.
\url{https://openreview.net/forum?id=rJVorjCcKQ}

\bibitem{lee2019}
T.~Lee, Z.~Lin, S.~Pushp, C.~Li, Y.~Liu, Y.~Lee, F.~Xu, C.~Xu, L.~Zhang, and J.~Song,
``Occlumency: Privacy-preserving remote deep-learning inference using SGX,''
in \emph{Proc.\ ACM MobiCom}, 2019.
\url{https://doi.org/10.1145/3300061.3345447}

\bibitem{intel2023}
Intel Corporation,
``Performance considerations: Intel Trust Domain Extensions on 4th Gen Intel Xeon Scalable Processors,''
2023.
\url{https://www.intel.com/content/www/us/en/developer/articles/technical/trust-domain-extensions-on-4th-gen-xeon-processors.html}

\bibitem{zhu2024}
J.~Zhu, H.~Yin, P.~Deng, A.~Almeida, and S.~Zhou,
``Confidential computing on NVIDIA Hopper GPUs: A performance benchmark study,''
arXiv preprint arXiv:2409.03992, 2024.
\url{https://arxiv.org/abs/2409.03992}

\bibitem{volos2018}
S.~Volos, K.~Vaswani, and R.~Bruno,
``Graviton: Trusted execution environments on GPUs,''
in \emph{Proc.\ USENIX OSDI}, 2018.
\url{https://www.usenix.org/conference/osdi18/presentation/volos}

\bibitem{jang2019}
I.~Jang, A.~Tang, T.~Kim, S.~Sethumadhavan, and J.~Huh,
``Heterogeneous isolated execution for commodity GPUs,''
in \emph{Proc.\ ACM ASPLOS}, 2019.
\url{https://doi.org/10.1145/3297858.3304021}

\bibitem{gilad2016}
R.~Gilad-Bachrach, N.~Dowlin, K.~Laine, K.~Lauter, M.~Naehrig, and J.~Wernsing,
``CryptoNets: Applying neural networks to encrypted data with high throughput and accuracy,''
in \emph{Proc.\ ICML}, 2016.
\url{https://proceedings.mlr.press/v48/gilad-bachrach16.html}

\bibitem{winter2023}
P.~Winter, R.~Giles, M.~Schafhuber, and H.~Haddadi,
``Nitriding: A tool kit for building scalable, networked, secure enclaves,''
arXiv preprint arXiv:2206.04123, 2023.
\url{https://arxiv.org/abs/2206.04123}

\bibitem{lutsch2025}
A.~Lutsch, C.~Franck, M.~El-Hindi, Z.~Istv\'an, and C.~Binnig,
``An analysis of AWS Nitro Enclaves for database workloads,''
in \emph{Proc.\ DaMoN (SIGMOD Workshop)}, 2025.
\url{https://doi.org/10.1145/3736227.3736234}

\bibitem{schnabl2025}
C.~Schnabl, D.~Hugenroth, B.~Marino, and A.~R.~Beresford,
``Attestable Audits: Verifiable AI safety benchmarks using trusted execution environments,''
arXiv preprint arXiv:2506.23706, 2025.
\url{https://arxiv.org/abs/2506.23706}

\end{thebibliography}

\end{document}
